
@article{aignerVisualMethodsAnalyzing2008,
  title = {Visual {{Methods}} for {{Analyzing Time-Oriented Data}}},
  author = {Aigner, Wolfgang and Miksch, Silvia and M{\"u}ller, Wolfgang and Schumann, Heidrun and Tominski, Christian},
  year = {2008},
  month = jan,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {14},
  number = {1},
  pages = {47--60},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2007.70415},
  abstract = {Providing appropriate methods to facilitate the analysis of time-oriented data is a key issue in many application domains. In this paper, we focus on the unique role of the parameter time in the context of visually driven data analysis.We will discuss three major aspects \textemdash{} visualization, analysis, and the user. It will be illustrated that it is necessary to consider the characteristics of time when generating visual representations.For that purpose we take a look at different types of time and present visual examples. Integrating visual and analytical methods has become an increasingly important issue. Therefore, we present our experiences in temporal data abstraction, principal component analysis, and clustering of larger volumes of time-oriented data. The third main aspect we discuss is supporting user-centered visual analysis.We describe event-based visualization as a promising means to adapt the visualization pipeline to needs and tasks of users.},
  keywords = {Analysis,Character generation,Data analysis,Data visualization,Displays,History,Pipelines,Principal component analysis,Project management,Taxonomy,Time-Oriented Data,User,Visualization},
  file = {/Users/moritz/Zotero/storage/AS9AMWD3/Aigner et al. - 2008 - Visual Methods for Analyzing Time-Oriented Data.pdf;/Users/moritz/Zotero/storage/29EGEBPM/4359494.html}
}

@misc{AnalyzingPopularRepositories2021,
  title = {Analyzing Popular Repositories on {{GitHub}}},
  year = {2021},
  month = jul,
  journal = {Analytics Vidhya},
  url = {https://www.analyticsvidhya.com/blog/2021/07/analyzing-popular-repositories-on-github/},
  urldate = {2022-04-13},
  abstract = {GitHub is one of the most popular version control and source code management platform. Here, we will be analyzing popular GitHub repositories},
  key = {Analyzing popular repositories on GitHub},
  langid = {english},
  keywords = {interesting data},
  file = {/Users/moritz/Zotero/storage/7V73YJUQ/analyzing-popular-repositories-on-github.html}
}

@phdthesis{anderlIdentifyingGitHubTrends2021,
  type = {Thesis},
  title = {Identifying {{GitHub Trends Using Temporal Analysis}}},
  author = {Anderl, Thomas},
  year = {2021},
  doi = {10.34726/hss.2021.93182},
  abstract = {Durch die COVID-19 Pandemie sowie dem steigenden Interesse an quelloffenen Projekten, gewinnen Versionskontrollsysteme wie Git an zunehmender Popularit\"at. Durch diesen Anstieg erh\"ohte sich auch die Vielfalt und das Ausma\ss{} an Daten auf Plattformen wie GitHub zunehmend, was zu steigendem Interesse f\"ur Soziologen und Softwareanalyst enf\"uhrt. Diese Arbeit fokussiert sich auf die Visualisierung von GitHub-Daten mit der Hilfe von Visual Analytics. Die Daten stammen sowohl aus der GitHub API als auch dem GitHub Archive, sind multivariate und enthalten diverse Informationen \"uber Projekte, Nutzer und Ereignisse. Diese Daten werden au\ss erdem durch die zeitliche Dimension erg\"anzt, um potentielle Trends zu entdecken. F\"ur die Problemdefinition und der Methodik wurde das Design Triangle wie von Miksch et. al beschrieben, herangezogen. Das Ergebnis dieser Arbeit ist ein Prototyp, der es Dom\"anen-Experten nicht nur erlaubt typische Aufgaben in Bezug auf GitHub Trends durchzuf\"uhren, sondern auch visuelle Interaktionsm\"oglichkeiten bereitstellt, um Fokus auf speziellere Zeitbereiche zu legen. Obwohl sich grunds\"atzlich viele Arten von Trends visualisieren lassen k\"onnten, fokussiert sich der hier entwickelte Prototyp nur auf eine kleinere Teilmenge von Problemen. Die generelle Zielgruppe liegt hierbei auf Analysten in technologischen Industrien. Der Prototyp wurde durch Dom\"anen-Experten mit verschiedenen Schwerpunkten durch eine vordefinierte Liste an Aufgaben evaluiert. Die Ergebnisse der Evaluation zeigen,dass es ein gro\ss es Interesse in der Analyse von GitHub-Daten gibt und das die Wahl der korrekten visuellen Kodierung und Interaktionsm\"oglichkeit essentiell f\"ur das Finden von Trends sein kann.},
  langid = {english},
  keywords = {sehr wichtig},
  file = {/Users/moritz/Zotero/storage/KM2K94AC/Anderl - 2021 - Identifying GitHub Trends Using Temporal Analysis.pdf}
}

@misc{ApacheECharts,
  title = {Apache {{ECharts}}},
  year = {2022},
  url = {https://echarts.apache.org/en/index.html},
  urldate = {2022-04-07},
  key = {Apache ECharts}
}

@misc{ApacheKibble2022,
  title = {Apache {{Kibble}}},
  year = {2022},
  month = apr,
  url = {https://github.com/apache/kibble},
  urldate = {2022-04-11},
  abstract = {Apache Kibble - a tool to collect, aggregate and visualize data about any software project},
  copyright = {Apache-2.0},
  key = {The Apache Software Foundation},
  keywords = {big-data,kibble,open-source,python,sehr wichtig,used,visualization}
}

@misc{Augur2022,
  title = {Augur},
  year = {2022},
  month = apr,
  url = {https://github.com/chaoss/augur},
  urldate = {2022-04-11},
  abstract = {Python library and web service for Open Source Software Health and Sustainability metrics \& data collection.},
  copyright = {MIT},
  key = {CHAOSS},
  keywords = {chaoss,data-collection,data-modeling,data-visualization,defined-metrics,facade,git,github,hacktoberfest,hacktoberfest2020,health,linux,linux-foundation,metrics,open-source,opensource,python-library,research,sehr wichtig,sustainability,unix,used}
}

@misc{bostockD3JsDataDriven,
  title = {D3.Js - {{Data-Driven Documents}}},
  author = {Bostock, Mike},
  year = {2022},
  url = {https://d3js.org/},
  urldate = {2022-04-07},
  abstract = {D3 is a JavaScript library for visualizing data with HTML, SVG, and CSS.},
  key = {D3.js - Data-Driven Documents}
}

@misc{cabot20ToolsHelp2021,
  title = {20+ Tools to Help You Mine and Analyze {{GitHub}} and {{Git}} Data},
  author = {Cabot, Jordi},
  year = {2021},
  month = aug,
  url = {https://livablesoftware.com/tools-mine-analyze-github-git-software-data/},
  urldate = {2022-04-07},
  abstract = {List of tools to mine, analyze and visualize all the data around your software projects, including users, commits, issues... from Git, GitHub and other popular platforms},
  key = {20+ tools to help you mine and analyze GitHub and Git data},
  keywords = {sehr wichtig,used}
}

@misc{canovasAllWeHave2017,
  title = {All We Have Learned about Software Development by Mining {{GitHub}} (plus Some Concerns)},
  author = {C{\'a}novas, Javier},
  year = {2017},
  month = may,
  url = {https://livablesoftware.com/learned-software-development-mining-github-plus-concerns/},
  urldate = {2022-04-10},
  abstract = {Mining information on software projects hosted on GitHub can reveal a lot of useful information on how software projects (and the community behind them) should be managed to optimize your chances of success},
  key = {All we have learned about software development by mining GitHub (plus some concerns)},
  keywords = {interesting data}
}

@inproceedings{canovasizquierdoGiLAGitHubLabel2015,
  title = {{{GiLA}}: {{GitHub}} Label Analyzer},
  shorttitle = {{{GiLA}}},
  booktitle = {2015 {{IEEE}} 22nd {{International Conference}} on {{Software Analysis}}, {{Evolution}}, and {{Reengineering}} ({{SANER}})},
  author = {C{\'a}novas Izquierdo, Javier Luis and Cosentino, Valerio and Rolandi, Bel{\'e}n and Bergel, Alexandre and Cabot, Jordi},
  year = {2015},
  month = mar,
  pages = {479--483},
  issn = {1534-5351},
  doi = {10.1109/SANER.2015.7081860},
  abstract = {Reporting bugs, asking for new features and in general giving any kind of feedback is a common way to contribute to an Open-Source Software (OSS) project. In GitHub, the largest code hosting service for OSS, this feedback is typically expressed as new issues for the project managed by an issue-tracking system available in each new project repository. Among other features, the issue tracker allows creating and assigning labels to issues with the goal of helping the project community to better classify and manage those issues (e.g., facilitating the identification of issues for top priority components or candidate developers that could solve them). Nevertheless, as the project grows a manual browsing of the project issues is no longer feasible. In this paper we present GiLA, a tool which generates a set of visualizations to facilitate the analysis of issues in a project depending on their label-based categorization. We believe our visualizations are useful to see the most popular labels (and their relationships) in a project, identify the most active community members for those labels and compare the typical issue evolution for each label category.},
  keywords = {Communities,Computer architecture,Databases,interesting data,Servers,Visualization,Web services},
  file = {/Users/moritz/Zotero/storage/VQYYFRI9/CÃ¡novas Izquierdo et al. - 2015 - GiLA GitHub label analyzer.pdf}
}

@misc{CauldronCauldron,
  title = {Cauldron / Cauldron},
  year = {2022},
  journal = {GitLab},
  url = {https://gitlab.com/cauldronio/cauldron},
  urldate = {2022-04-12},
  abstract = {Project to manage Cauldron issues},
  key = {Cauldron / cauldron},
  langid = {english},
  keywords = {sehr wichtig,used},
  file = {/Users/moritz/Zotero/storage/9C6RN6C8/cauldron.html}
}

@misc{CauldronCloud,
  title = {Cauldron {{Cloud}}},
  year = {2022},
  url = {https://cloud.cauldron.io/},
  urldate = {2022-04-12},
  abstract = {Open source projects development analytics},
  key = {Cauldron Cloud},
  langid = {english},
  keywords = {sehr wichtig,used},
  file = {/Users/moritz/Zotero/storage/5B8PN47U/cloud.cauldron.io.html}
}

@misc{CHAOSSCommonMetrics2022,
  title = {{{CHAOSS Common Metrics Working Group}}},
  year = {2022},
  month = apr,
  url = {https://github.com/chaoss/wg-common},
  urldate = {2022-04-11},
  abstract = {CHAOSS Common Metrics Working Group},
  copyright = {MIT},
  key = {CHAOSS},
  keywords = {sehr wichtig,used}
}

@misc{ChartJsOpen,
  title = {Chart.Js | {{Open}} Source {{HTML5 Charts}} for Your Website},
  year = {2022},
  url = {https://www.chartjs.org/},
  urldate = {2022-04-07},
  key = {Chart.js | Open source HTML5 Charts for your website}
}

@inproceedings{chatziasimidisDataCollectionAnalysis2015,
  title = {Data Collection and Analysis of {{GitHub}} Repositories and Users},
  booktitle = {2015 6th {{International Conference}} on {{Information}}, {{Intelligence}}, {{Systems}} and {{Applications}} ({{IISA}})},
  author = {Chatziasimidis, Fragkiskos and Stamelos, Ioannis},
  year = {2015},
  month = jul,
  pages = {1--6},
  doi = {10.1109/IISA.2015.7388026},
  abstract = {In this paper, we present the collection and mining of GitHub data, aiming to understand GitHub user behavior and project success factors. We collected information about approximately 100K projects and 10K GitHub users//owners of these projects, via GitHub API. Subsequently, we statistically analyzed such data, discretized values of features via k-means algorithm, and finally we applied apriori algorithm via weka in order to find out association rules. Having assumed that project success could be measured by the cardinality of downloads we kept only the rules which had as right par a download cardinality higher than a threshold of 1000 downloads. The results provide intersting insight in the GitHub ecosystem and seven success rules for GitHub projects.},
  keywords = {association rules,discretization,GitHub,GitHub API,interesting data,k-means,open source,project success},
  file = {/Users/moritz/Zotero/storage/Q74RGFWB/Chatziasimidis and Stamelos - 2015 - Data collection and analysis of GitHub repositorie.pdf;/Users/moritz/Zotero/storage/PB9A99WT/7388026.html}
}

@article{duenasGrimoireLabToolsetSoftware2021,
  title = {{{GrimoireLab}}: {{A}} Toolset for Software Development Analytics},
  shorttitle = {{{GrimoireLab}}},
  author = {Due{\~n}as, Santiago and Cosentino, Valerio and {Gonzalez-Barahona}, Jesus M. and Felix, Alvaro del Castillo San and {Izquierdo-Cortazar}, Daniel and {Ca{\~n}as-D{\'i}az}, Luis and {Garc{\'i}a-Plaza}, Alberto P{\'e}rez},
  year = {2021},
  month = jul,
  journal = {PeerJ Comput. Sci.},
  volume = {7},
  pages = {e601},
  publisher = {{PeerJ Inc.}},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.601},
  abstract = {Background After many years of research on software repositories, the knowledge for building mature, reusable tools that perform data retrieval, storage and basic analytics is readily available. However, there is still room to improvement in the area of reusable tools implementing this knowledge. Goal To produce a reusable toolset supporting the most common tasks when retrieving, curating and visualizing data from software repositories, allowing for the easy reproduction of data sets ready for more complex analytics, and sparing the researcher or the analyst of most of the tasks that can be automated. Method Use our experience in building tools in this domain to identify a collection of scenarios where a reusable toolset would be convenient, and the main components of such a toolset. Then build those components, and refine them incrementally using the feedback from their use in both commercial, community-based, and academic environments. Results GrimoireLab, an efficient toolset composed of five main components, supporting about 30 different kinds of data sources related to software development. It has been tested in many environments, for performing different kinds of studies, and providing different kinds of services. It features a common API for accessing the retrieved data, facilities for relating items from different data sources, semi-structured storage for easing later analysis and reproduction, and basic facilities for visualization, preliminary analysis and drill-down in the data. It is also modular, making it easy to support new kinds of data sources and analysis. Conclusions We present a mature toolset, widely tested in the field, that can help to improve the situation in the area of reusable tools for mining software repositories. We show some scenarios where it has already been used. We expect it will help to reduce the effort for doing studies or providing services in this area, leading to advances in reproducibility and comparison of results.},
  langid = {english},
  keywords = {sehr wichtig,used},
  file = {/Users/moritz/Zotero/storage/GIJ3U3TR/DueÃ±as et al. - 2021 - GrimoireLab A toolset for software development an.pdf;/Users/moritz/Zotero/storage/KK454PX7/cs-601.html}
}

@misc{EjwaGitinspector2022,
  title = {Ejwa/Gitinspector},
  year = {2022},
  month = apr,
  url = {https://github.com/ejwa/gitinspector},
  urldate = {2022-04-11},
  abstract = {:bar\_chart: The statistical analysis tool for git repositories},
  copyright = {GPL-3.0},
  key = {Ejwa Software},
  keywords = {analysis,git,gitinspector,sehr wichtig,statistical-analysis,statistics,timeline-analysis,used}
}

@misc{FrontendRepoAnalyzer2022,
  title = {Frontend {{Repo Analyzer}}},
  year = {2022},
  month = feb,
  url = {https://github.com/feedzai/repo-analyzer},
  urldate = {2022-04-11},
  copyright = {MIT},
  key = {Feedzai},
  keywords = {sehr wichtig,used}
}

@misc{GitHubRepoAnalysisa,
  title = {{{GitHub Repo Analysis}}},
  year = {2022},
  url = {https://kaggle.com/tsalitzion/github-repo-analysis},
  urldate = {2022-04-11},
  abstract = {Explore and run machine learning code with Kaggle Notebooks | Using data from Top 980 Starred Open Source Projects on GitHub},
  key = {GitHub Repo Analysis},
  langid = {english},
  keywords = {interesting data},
  file = {/Users/moritz/Zotero/storage/HSMWU5BJ/notebook.html}
}

@misc{GrimoireLab2022,
  title = {{{GrimoireLab}}},
  year = {2022},
  month = apr,
  url = {https://github.com/chaoss/grimoirelab},
  urldate = {2022-04-11},
  abstract = {GrimoireLab: toolset for software development analytics},
  copyright = {GPL-3.0},
  key = {CHAOSS},
  keywords = {chaoss,data-mining,data-visualization,grimoirelab,hacktoberfest,metrics,sehr wichtig,software-analytics,used}
}

@article{guoSurveyVisualAnalysis2021a,
  title = {A {{Survey}} on {{Visual Analysis}} of {{Event Sequence Data}}},
  author = {Guo, Yi and Guo, Shunan and Jin, Zhuochen and Kaul, Smiti and Gotz, David and Cao, Nan},
  year = {2021},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  pages = {1--1},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2021.3100413},
  abstract = {Event sequence data record series of discrete events in the time order of occurrence. They are commonly observed in a variety of applications ranging from electronic health records to network logs, with the characteristics of large-scale, high-dimensional and heterogeneous. This high complexity of event sequence data makes it difficult for analysts to manually explore and find patterns, resulting in ever-increasing needs for computational and perceptual aids from visual analytics techniques to extract and communicate insights from event sequence datasets. In this paper, we review the state-of-the-art visual analytics approaches, characterize them with our proposed design space, and categorize them based on analytical tasks and applications.},
  keywords = {Data mining,Data visualization,Event Sequences,Medical diagnostic imaging,Pipelines,Sequences,Task analysis,Visual Analysis,Visual analytics,Visualization},
  file = {/Users/moritz/Zotero/storage/4D5TCSYJ/Guo et al. - 2021 - A Survey on Visual Analysis of Event Sequence Data.pdf}
}

@misc{HaystackAnalyticsEngineering,
  title = {Haystack - {{Analytics}} for {{Engineering Leaders}}},
  year = {2022},
  url = {https://www.usehaystack.io/?utm_campaign=Use-Cases%20Campaign&utm_source=google&utm_medium=cpc&utm_content=GitHub%20Dashboard&utm_term=git%20repository%20statistics&gclid=EAIaIQobChMI4efY64OC9wIViIbVCh2aIgXuEAMYASAAEgIC0_D_BwE},
  urldate = {2022-04-07},
  abstract = {Visibility Into How Your Software Team Works.  Insights, Alerts and Software Development Metrics from Github.},
  key = {Haystack - Analytics for Engineering Leaders},
  keywords = {sehr wichtig,used}
}

@misc{Insights,
  title = {Insights},
  year = {2022},
  url = {https://insights.lfx.linuxfoundation.org/projects/korg/dashboard;quicktime=time_filter_3Y},
  urldate = {2022-04-11},
  key = {Insights},
  keywords = {sehr wichtig,used},
  file = {/Users/moritz/Zotero/storage/QUELMS9L/dashboard\;quicktime=time_filter_3Y.html}
}

@article{jagadeeshchandraboseProcessDiagnosticsUsing2012,
  title = {Process Diagnostics Using Trace Alignment: {{Opportunities}}, Issues, and Challenges},
  shorttitle = {Process Diagnostics Using Trace Alignment},
  author = {Jagadeesh Chandra Bose, R. P. and {\noopsort{aalst}}{van der Aalst}, Wil M. P.},
  year = {2012},
  month = apr,
  journal = {Information Systems},
  series = {Management and {{Engineering}} of {{Process-Aware Information Systems}}},
  volume = {37},
  number = {2},
  pages = {117--141},
  issn = {0306-4379},
  doi = {10.1016/j.is.2011.08.003},
  abstract = {Business processes leave trails in a variety of data sources (e.g., audit trails, databases, and transaction logs). Hence, every process instance can be described by a trace, i.e., a sequence of events. Process mining techniques are able to extract knowledge from such traces and provide a welcome extension to the repertoire of business process analysis techniques. Recently, process mining techniques have been adopted in various commercial BPM systems (e.g., BPM|one, Futura Reflect, ARIS PPM, Fujitsu Interstage, Businesscape, Iontas PDF, and QPR PA). Unfortunately, traditional process discovery algorithms have problems dealing with less structured processes. The resulting models are difficult to comprehend or even misleading. Therefore, we propose a new approach based on trace alignment. The goal is to align traces in such a way that event logs can be explored easily. Trace alignment can be used to explore the process in the early stages of analysis and to answer specific questions in later stages of analysis. Hence, it complements existing process mining techniques focusing on discovery and conformance checking. The proposed techniques have been implemented as plugins in the ProM framework. We report the results of trace alignment on one synthetic and two real-life event logs, and show that trace alignment has significant promise in process diagnostic efforts.},
  langid = {english},
  keywords = {Alignment,Conformance,Diagnostics,Execution patterns,Process mining},
  file = {/Users/moritz/Zotero/storage/SVQBX2NQ/Jagadeesh Chandra Bose and van der Aalst - 2012 - Process diagnostics using trace alignment Opportu.pdf}
}

@inproceedings{kumarj.DataVisualizationGitHub2018,
  title = {Data {{Visualization}} on {{GitHub Repository Parameters Using Elastic Search}} and {{Kibana}}},
  booktitle = {2018 2nd {{International Conference}} on {{Trends}} in {{Electronics}} and {{Informatics}} ({{ICOEI}})},
  author = {Kumar J., Mohan and Dubey, Shishir and Balaji, B. and Rao, Dinesh and Rao, Deepak},
  year = {2018},
  month = may,
  pages = {554--558},
  doi = {10.1109/ICOEI.2018.8553755},
  abstract = {Data Visualization makes data mean more through storytelling. Any data will have an inside story when it is addressed with relevant query. In this paper, GitHub data is collected, cleansed and visualized for its repository parameters on various questions. GitHub being the platform for many developers is not only used for version controlling their project but also for sharing it. Through this technique the inside story and the use of GitHub is seen.},
  keywords = {Big Data,Companies,Conferences,Data visualization,Data Visualization,GitHub,Image color analysis,interesting data,Market research,Software,Urban areas},
  file = {/Users/moritz/Zotero/storage/TK63AXSS/Kumar J. et al. - 2018 - Data Visualization on GitHub Repository Parameters.pdf}
}

@misc{LevelSoftwareDevelopment,
  title = {Level up {{Software Development Analytics}} - {{Cauldron}}},
  year = {2022},
  url = {https://cauldron.io/},
  urldate = {2022-04-10},
  abstract = {Cauldron allows project managers, analysts, and developers to understand more about the community and processes involved in software development},
  key = {Level up Software Development Analytics - Cauldron},
  keywords = {sehr wichtig,used}
}

@article{liaoExploringCharacteristicsIssueRelated2018,
  title = {Exploring the {{Characteristics}} of {{Issue-Related Behaviors}} in {{GitHub Using Visualization Techniques}}},
  author = {Liao, Zhifang and He, Dayu and Chen, Zhijie and Fan, Xiaoping and Zhang, Yan and Liu, Shengzong},
  year = {2018},
  journal = {IEEE Access},
  volume = {6},
  pages = {24003--24015},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2810295},
  abstract = {Feedback from software users, such as bug reports, is vital in the management of software projects. In GitHub, the feedback is typically expressed as new issues. Through filing issue reports, users may help identify and fix bugs, document software code, and enhance software quality via feature requests. In this paper, we aim at investigating some characteristics of issues to facilitate issue management and software management. We investigate the important degrees of behaviors that are related to issues in popular projects to assess the importance of issues in GitHub and analyze the effectiveness of issue labeling for issue handling. Then, we explore the patterns of issue commits over time in popular projects based on visual analysis and obtain the following results: we find that the behaviors that are related to issues play important roles in the GitHub. We also find that the time distribution of issue commits follows a three-period development model, which approximately corresponds to the project life cycle. These results may provide a new knowledge about issues that can help managers manage and allocate project resources more effectively and even reduce software failures.},
  keywords = {Analytical models,Computer bugs,Data visualization,interesting data,issue commit,Labeling,Open-source software community,project development model,Sentiment analysis,Software,software management,visual analysis,Visualization},
  file = {/Users/moritz/Zotero/storage/X4EVT6XB/Liao et al. - 2018 - Exploring the Characteristics of Issue-Related Beh.pdf;/Users/moritz/Zotero/storage/7V6QXMAH/8304571.html}
}

@article{liEChartsDeclarativeFramework2018,
  title = {{{ECharts}}: {{A}} Declarative Framework for Rapid Construction of Web-Based Visualization},
  shorttitle = {{{ECharts}}},
  author = {Li, Deqing and Mei, Honghui and Shen, Yi and Su, Shuang and Zhang, Wenli and Wang, Junting and Zu, Ming and Chen, Wei},
  year = {2018},
  month = jun,
  journal = {Visual Informatics},
  volume = {2},
  number = {2},
  pages = {136--146},
  issn = {2468-502X},
  doi = {10.1016/j.visinf.2018.04.011},
  abstract = {While there have been a dozen of authoring systems and programming toolkits for visual design and development, users who do not have programming skills, such as data analysts or interface designers, still may feel cumbersome to efficiently implement a web-based visualization. In this paper, we present ECharts, an open-sourced, web-based, cross-platform framework that supports the rapid construction of interactive visualization. The motivation is driven by three goals: easy-to-use, rich built-in interactions, and high performance. The kernel of ECharts is a suite of declarative visual design language that customizes built-in chart types. The underlying streaming architecture, together with a high-performance graphics renderer based on HTML5 canvas, enables the high expandability and performance of ECharts. We report the design, implementation, and applications of ECharts with a diverse variety of examples. We compare the utility and performance of ECharts with C3.js, HighCharts, and Chart.js. Results of the experiments demonstrate the efficiency and scalability of our framework. Since the first release in June 2013, ECharts has iterated 63 versions, and attracted over 22,000 star counts and over 1700 related projects in the GitHub. ECharts is regarded as a leading visualization development tool in the world, and ranks the third in the GitHub visualization tab.},
  langid = {english},
  keywords = {Information visualization,Web-based visualization},
  file = {/Users/moritz/Zotero/storage/2A4XFPBU/Li et al. - 2018 - ECharts A declarative framework for rapid constru.pdf}
}

@inproceedings{ludwigCompilingStaticSoftware2017,
  title = {Compiling Static Software Metrics for Reliability and Maintainability from {{GitHub}} Repositories},
  booktitle = {2017 {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{SMC}})},
  author = {Ludwig, Jeremy and Xu, Steven and Webber, Frederick},
  year = {2017},
  month = oct,
  pages = {5--9},
  doi = {10.1109/SMC.2017.8122569},
  abstract = {This paper identifies a small, essential set of static software code metrics linked to the software product quality characteristics of reliability and maintainability and to the most commonly identified sources of technical debt. A plug-in is created for the Understand code visualization and static analysis tool that calculates and aggregates the metrics. The plug-in produces a high-level interactive html report as well as developer-level information needed to address quality issues using Understand. A script makes use of Git, Understand, and the plug-in to compile results for a list of GitHub repositories into a single file. The primary contribution of this work is to describe an open-source plug-in to measure and visualize architectural complexity based on the propagation cost and core size metrics, which are not currently found in other tools. The plug-in should be useful to researchers and practitioners interested in these two metrics and as an expedient starting point to experimentation with metric collection and aggregation for groups of GitHub repositories. The plug-in was developed as a first step in an ongoing project aimed at applying case-based reasoning to the issue of software product quality.},
  keywords = {architecture,Complexity theory,interesting data,maintainability,Measurement,metrics,reliability,software product quality,Software quality,Software reliability,static code analysis,technical debt,Tools},
  file = {/Users/moritz/Zotero/storage/VSCL8SPK/Ludwig et al. - 2017 - Compiling static software metrics for reliability .pdf}
}

@misc{Monocle2022,
  title = {Monocle},
  year = {2022},
  month = apr,
  url = {https://github.com/change-metrics/monocle},
  urldate = {2022-04-12},
  abstract = {Monocle helps teams and individual to better organize daily duties and to detect anomalies in the way changes are produced and reviewed.},
  copyright = {AGPL-3.0},
  key = {change-metrics},
  keywords = {analytics,change,changes,gerrit,github,gitlab,graphql,metric,pull-requests,reviews,sehr wichtig,software-analytics,software-engineering,stats,used}
}

@misc{RepoSense2022,
  title = {{{RepoSense}}},
  year = {2022},
  month = apr,
  url = {https://github.com/reposense/RepoSense},
  urldate = {2022-04-11},
  abstract = {Contribution analysis tool for Git repositories},
  copyright = {MIT},
  key = {RepoSense},
  keywords = {author-statistics,code-review,contribution-analysis,git-statistics,repository-management,sehr wichtig,used}
}

@misc{RepoSenseHome,
  title = {{{RepoSense}} - {{Home}}},
  year = {2022},
  url = {https://reposense.org/index.html},
  urldate = {2022-04-12},
  key = {RepoSense - Home},
  keywords = {sehr wichtig,used},
  file = {/Users/moritz/Zotero/storage/5UZKQ3QQ/index.html}
}

@inproceedings{ruskLocationBasedAnalysisDevelopers2014,
  title = {Location-{{Based Analysis}} of {{Developers}} and {{Technologies}} on {{GitHub}}},
  booktitle = {2014 28th {{International Conference}} on {{Advanced Information Networking}} and {{Applications Workshops}}},
  author = {Rusk, David and Coady, Yvonne},
  year = {2014},
  month = may,
  pages = {681--685},
  doi = {10.1109/WAINA.2014.110},
  abstract = {GitHub is a popular platform for collaboration on open source projects. It also provides a rich API to query various aspects of the public activity. This combination of a popular social coding website with a rich API presents an opportunity for researchers to gather empirical data about software development practices. There are an overwhelmingly large number of competing platforms to choose from in software development. Knowing which are gaining widespread adoption is valuable both for individual developers trying to increase their employability, as well as software engineers deciding which technology to use in their next big project. In terms of a developer's employability and an employer's ability to find available developers in their economic region, it is important to identify the most common technologies by geographic location. In this paper, analyses are done on GitHub data taking into account the developers' location and their technology usage. A web-based tool has been developed to interact with and visualize this data. In its current state of development, the tool summarizes the amount of code developers have in their public repositories broken down by programming language, and summarizes data about programmers using specific programming languages. This allows website visitors to get an immediate picture of the programming language usage in their region of interest. Future research could expand this work to technologies beyond programming languages such as frameworks and libraries.},
  keywords = {Companies,Computer languages,Data visualization,Educational institutions,Electronic mail,Encoding,GitHub,interesting data,open source,programming languages,REST API,Software,software repository},
  file = {/Users/moritz/Zotero/storage/UEG6FGL2/Rusk and Coady - 2014 - Location-Based Analysis of Developers and Technolo.pdf}
}

@book{schumannVisualisierung2000,
  title = {{Visualisierung}},
  author = {Schumann, Heidrun and M{\"u}ller, Wolfgang},
  year = {2000},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-57193-0},
  isbn = {978-3-540-64944-1 978-3-642-57193-0},
  langid = {german},
  file = {/Users/moritz/Zotero/storage/MI4CNPGX/Schumann and MÃ¼ller - 2000 - Visualisierung.pdf}
}

@incollection{shneidermanEyesHaveIt2003,
  title = {The {{Eyes Have It}}: {{A Task}} by {{Data Type Taxonomy}} for {{Information Visualizations}}},
  shorttitle = {The {{Eyes Have It}}},
  booktitle = {The {{Craft}} of {{Information Visualization}}},
  author = {Shneiderman, Ben},
  editor = {Bederson, BENJAMIN B. and Shneiderman, BEN},
  year = {2003},
  month = jan,
  series = {Interactive {{Technologies}}},
  pages = {364--371},
  publisher = {{Morgan Kaufmann}},
  address = {{San Francisco}},
  doi = {10.1016/B978-155860915-0/50046-9},
  abstract = {A useful starting point for designing advanced graphical user interfaces is the Visual Information-Seeking Mantra: overview first, zoom and filter, then details on demand. But this is only a starting point in trying to understand the rich and varied set of information visualizations that have been proposed in recent years. This paper offers a task by data type taxonomy with seven data types (one-, two-, three-dimensional data, temporal and multi-dimensional data, and tree and network data) and seven tasks (overview, zoom, filter, details-on-demand, relate, history, and extracts).Everything points to the conclusion that the phrase `the language of art' is more than a loose metaphor, that even to describe the visible world in images we need a developed system of schemata. E. H. Gombrich Art and Illusion, 1959 (p. 76)},
  isbn = {978-1-55860-915-0},
  langid = {english},
  file = {/Users/moritz/Zotero/storage/PR7BKIWT/Shneiderman - 2003 - The Eyes Have It A Task by Data Type Taxonomy for.pdf;/Users/moritz/Zotero/storage/KMFMGDBI/B9781558609150500469.html}
}

@inproceedings{sundarAnalyzingPredictingLifetime2015,
  title = {Analyzing and Predicting {{Lifetime}} of Trends Using Social Networks},
  booktitle = {2015 {{International Conference}} on {{Computer Communication}} and {{Informatics}} ({{ICCCI}})},
  author = {Sundar, D. Sam and Kankanala, Mila},
  year = {2015},
  month = jan,
  pages = {1--7},
  doi = {10.1109/ICCCI.2015.7218090},
  abstract = {Trending topics in any social networking site are those specific trends that are most popular and talked about at that point in time. The emergence, survival and eventual fading away of a trend are a function of various factors. In this paper, an exhaustive list of determinants is pioneered with respect to the survival of a trend. Using the first determinant, Interest Over Time, and utilizing sentiment analysis over the textual data associated with the trend, the longevity of a trend is predicted. Additionally, a modified and more accurate mathematical equation for Trend Survival Score is formulated where Predictive Analysis and Modeling with First Order Differential Equations is done with the other two determinants. In order to map the Lifetime Score to a timeframe relevant to real-world scenario, K-nearest neighbor approach is used (using k=3) considering how lifetime score of a trend converts to actual lifetime in the training data.},
  keywords = {Computers,determinants,Informatics,Market research,Mathematical model,Predictive Analysis,sentiment analysis,Sentiment analysis,text mining,Trend Survival Score,trends,Twitter},
  file = {/Users/moritz/Zotero/storage/Q28RWJ6X/Sundar and Kankanala - 2015 - Analyzing and predicting Lifetime of trends using .pdf;/Users/moritz/Zotero/storage/6DFNR48N/7218090.html}
}

@article{vanderaalstHowWriteBeautiful2022,
  title = {How to {{Write Beautiful Process-and-Data-Science Papers}}?},
  author = {{\noopsort{aalst}}{van der Aalst}, Wil M. P.},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.09286 [cs]},
  eprint = {2203.09286},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2203.09286},
  urldate = {2022-03-22},
  abstract = {After 25 years of PhD supervision, the author noted typical recurring problems that make papers look sloppy, difficult to read, and incoherent. The goal is not to write a paper for the sake of writing a paper, but to convey a valuable message that is clear and precise. The goal is to write papers that have an impact and are still understandable a couple of decades later. Our mission should be to create papers of high quality that people want to read and that can stand the test of time. We use Dijkstra's adagium "Beauty Is Our Business" to stress the importance of simplicity, correctness, and cleanness.},
  archiveprefix = {arXiv},
  keywords = {A.0,Computer Science - Digital Libraries,K.3.0},
  file = {/Users/moritz/Zotero/storage/RC9IJGG5/van der Aalst - 2022 - How to Write Beautiful Process-and-Data-Science Pa.pdf;/Users/moritz/Zotero/storage/TS3JPLMX/2203.html}
}

@misc{VegaLite2022,
  title = {Vega-{{Lite}}},
  year = {2022},
  month = apr,
  url = {https://github.com/vega/vega-lite},
  urldate = {2022-04-11},
  abstract = {A concise grammar of interactive graphics, built on Vega.},
  copyright = {BSD-3-Clause},
  key = {Vega},
  keywords = {charts,declarative-language,plot,sehr wichtig,vega,vega-lite,visual-analysis,visualization,visualization-grammar}
}

@misc{VegaVisualizationGrammar2022,
  title = {Vega: {{A Visualization Grammar}}},
  shorttitle = {Vega},
  year = {2022},
  month = apr,
  url = {https://github.com/vega/vega},
  urldate = {2022-04-11},
  abstract = {A visualization grammar.},
  copyright = {BSD-3-Clause},
  key = {Vega},
  keywords = {canvas,d3,svg,vega,visualization,visualization-grammar}
}

@inproceedings{weichengMiningGitHubWhy2013,
  title = {Mining {{GitHub}}: {{Why Commit Stops}} \textendash{} {{Exploring}} the {{Relationship}} between {{Developer}}'s {{Commit Pattern}} and {{File Version Evolution}}},
  shorttitle = {Mining {{GitHub}}},
  booktitle = {2013 20th {{Asia-Pacific Software Engineering Conference}} ({{APSEC}})},
  author = {Weicheng, Yang and Beijun, Shen and Ben, Xu},
  year = {2013},
  month = dec,
  volume = {2},
  pages = {165--169},
  issn = {1530-1362},
  doi = {10.1109/APSEC.2013.133},
  abstract = {Using the freeware in GitHub, we are often confused by a phenomenon: the new version of GitHub freeware usually was released in an indefinite frequency, and developers often committed nothing for a long time. This evolution phenomenon interferes with our own development plan and architecture design. Why do these updates happen at that time? Can we predict GitHub software version evolution by developers' activities? This paper aims to explore the developer commit patterns in GitHub, and try to mine the relationship between these patterns (if exists) and code evolution. First, we define four metrics to measure commit activity and code evolution: the changes in each commit, the time between two commits, the author of each changes, and the source code dependency. Then, we adopt visualization techniques to explore developers' commit activity and code evolution. Visual techniques are used to describe the progress of the given project and the authors' contributions. To analyze the commit logs in GitHub software repository automatically, Commits Analysis Tool (CAT) is designed and implemented. Finally, eight open source projects in GitHub are analyzed using CAT, and we find that: 1) the file changes in the previous versions may affect the file depend on it in the next version, 2) the average days around "huge commit" is 3 times of that around normal commit. Using these two patterns and developer's commit model, we can predict when his next commit comes and which file may be changed in that commit. Such information is valuable for project planning of both GitHub projects and other projects which use GitHub freeware to develop software.},
  keywords = {commit pattern,Data mining,Data visualization,GitHub,History,interesting data,Java,Measurement,repository mining,Software,version evolution,Visualization,visualization technology},
  file = {/Users/moritz/Zotero/storage/SWGTXIKC/Weicheng et al. - 2013 - Mining GitHub Why Commit Stops â Exploring the Re.pdf}
}

@article{yeshchenkoSurveyApproachesEvent2022,
  title = {A {{Survey}} of {{Approaches}} for {{Event Sequence Analysis}} and {{Visualization}} Using the {{ESeVis Framework}}},
  author = {Yeshchenko, Anton and Mendling, Jan},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.07941 [cs]},
  eprint = {2202.07941},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2202.07941},
  urldate = {2022-03-22},
  abstract = {Event sequence data is increasingly available. Many business operations are supported by information systems that record transactions, events, state changes, message exchanges, and so forth. This observation is equally valid for various industries, including production, logistics, healthcare, financial services, education, to name but a few. The variety of application areas explains that techniques for event sequence data analysis have been developed rather independently in different fields of computer science. Most prominent are contributions from information visualization and from process mining. So far, the contributions from these two fields have neither been compared nor have they been mapped to an integrated framework. In this paper, we develop the Event Sequence Visualization framework (ESeVis) that gives due credit to the traditions of both fields. Our mapping study provides an integrated perspective on both fields and identifies potential for synergies for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/Users/moritz/Zotero/storage/83CWJBKX/Yeshchenko and Mendling - 2022 - A Survey of Approaches for Event Sequence Analysis.pdf;/Users/moritz/Zotero/storage/CLK3FDIE/2202.html}
}

@preamble{ "\newcommand{\noopsort}[1]{} " }

